{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMndW0EC/s3qS9jNZ+XBYWS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swalehaparvin/HuggingFace/blob/main/Hugging_face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment analysis using a pre-trained DistilBERT model fine-tuned on the SST-2 dataset\n",
        "\n",
        "The model outputs either:\n",
        "\n",
        "POSITIVE (confidence score typically > 0.5)\n",
        "\n",
        "NEGATIVE (confidence score typically < 0.5)\n",
        "\n",
        "Loads a tokenizer and model specifically trained for sentiment classification\n",
        "\n",
        "Achieves 91.3% accuracy on SST-2 validation set\n",
        "\n",
        "Tokenizes input text into model-readable format\n",
        "\n",
        "return_tensors=\"pt\" specifies PyTorch tensors\n",
        "\n",
        "Disables gradient calculation for inference efficiency\n",
        "\n",
        "Generates prediction scores (logits)\n",
        "Selects the highest confidence score\n",
        "\n",
        "Maps numerical ID to human-readable label\n",
        "\n"
      ],
      "metadata": {
        "id": "y8CsIxL01Zrg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peeYGg7lROko"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "inputs = tokenizer(\"Hello, I like america\", return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "predicted_class_id = logits.argmax().item()\n",
        "model.config.id2label[predicted_class_id]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "!pip install huggingface_hub\n",
        "text = \"AI-powered robots assist in complex brain surgeries with precision.\"\n",
        "\n",
        "# Create the pipeline\n",
        "classifier = pipeline(task=\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Create the categories list\n",
        "categories = [\"politics\", \"science\", \"sports\"]\n",
        "\n",
        "# Predict the output\n",
        "output = classifier(text,categories)\n",
        "\n",
        "# Print the top label and its score\n",
        "print(f\"Top Label: {output['labels'][0]} with score: {output['scores'][0]}\")"
      ],
      "metadata": {
        "id": "hSjVJGK-k6Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate and display the dataset size in megabytes"
      ],
      "metadata": {
        "id": "DKr5lZwF0mfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function to load dataset metadata\n",
        "from datasets import load_dataset_builder\n",
        "# Initialize the dataset builder for the MMLU-Pro dataset and Display dataset metadata\n",
        "reviews_builder = load_dataset_builder(\"TIGER-Lab/MMLU-Pro\")\n",
        "print(reviews_builder.info)\n",
        "# Calculate and print the dataset size in MB\n",
        "dataset_size_mb = reviews_builder.info.dataset_size / (1024 ** 2)\n",
        "print(f\"Dataset size: {round(dataset_size_mb, 2)} MB\")\n"
      ],
      "metadata": {
        "id": "OvnvghEa0nVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manipulating datasets\n",
        "There will likely be many occasions when you will need to manipulate a dataset before using it within a ML task. Two common manipulations are filtering and selecting (or slicing). The dataset is already loaded for you under wikipedia.\n",
        "Filter the dataset for rows with the term \"football\" in the text column and save as filtered.\n",
        "Select a single example from the filtered dataset and save as example."
      ],
      "metadata": {
        "id": "pWfiSGc80r8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "wikipedia = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
        "# Filter the documents\n",
        "filtered = wikipedia.filter(lambda row: \"football\" in row[\"text\"])\n",
        "# Create a sample dataset\n",
        "example = filtered.select(range(1))\n",
        "print(example[0][\"text\"])"
      ],
      "metadata": {
        "id": "Eoj4LKKj0uyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grammatical correctness\n",
        "Text classification is the process of labeling an input text into a pre-defined category. This can take the form of sentiment - positive or negative - spam detection - spam or not spam - and even grammatical errors.\n",
        "Explore the use of a text-classification pipeline for checking an input sentence for grammatical errors.\n"
      ],
      "metadata": {
        "id": "akZRSBOi1FL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline for grammar checking\n",
        "grammar_checker = pipeline(\n",
        "  task=\"text-classification\",\n",
        "  model=\"abdulmatinomotoso/English_Grammar_Checker\"\n",
        ")\n",
        "# Check grammar of the input text\n",
        "output = grammar_checker(\"I will walk dog\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "TSIN5NAz1G0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Natural Language Inference\n",
        "Another task under the text classification umbrella is Question Natural Language Inference, or QNLI. This checks if a premise contains enough information to answer a posed question, determining whether the answer can be found in the given text.\n",
        "Create a text classification QNLI pipeline using the model \"cross-encoder/qnli-electra-base\" and save as classifier.\n",
        "Use this classifier to determine if the text provides enough information to answer the question."
      ],
      "metadata": {
        "id": "AI8S_doQ1O11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the pipeline\n",
        "classifier=pipeline(task=\"text-classification\", model=\"cross-encoder/qnli-electra-base\")\n",
        "# Predict the output\n",
        "output = classifier(\"Where is the capital of France?, Brittany is known for its stunning coastline.\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "wkExL8_41NkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic category assignment\n",
        "Dynamic category assignment enables a model to classify text into predefined categories, even without prior training for those categories.\n",
        "Build the pipeline and save as a classifier.\n",
        "Create a list of the labels - \"politics\", \"science\", \"sports\" - and save as categories.\n",
        "Predict the label of text using the classifier and predefined categories."
      ],
      "metadata": {
        "id": "qYs7PWno1quw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"AI-powered robots assist in complex brain surgeries with precision.\"\n",
        "# Create the pipeline\n",
        "classifier = pipeline(task=\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "# Create the categories list\n",
        "categories = [\"politics\", \"science\", \"sports\"]\n",
        "# Predict the output\n",
        "output = classifier(text,categories)\n",
        "# Print the top label and its score\n",
        "print(f\"Top Label: {output['labels'][0]} with score: {output['scores'][0]}\")"
      ],
      "metadata": {
        "id": "Y_jhqSZP1sIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarizing long text\n",
        "Summarization reduces large text into manageable content, helping readers quickly grasp key points from lengthy articles or documents.\n",
        "There are two main types: extractive, which selects key sentences from the original text, and abstractive, which generates new sentences summarizing main ideas."
      ],
      "metadata": {
        "id": "MnLfgLWz1xWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the summarization pipeline\n",
        "summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\")\n",
        "# Summarize the text\n",
        "summary_text = summarizer(text)\n",
        "# Compare the length\n",
        "print(f\"Original text length: {len(text)}\")\n",
        "print(f\"Summary length: {len(summary_text[0]['summary_text'])}\")"
      ],
      "metadata": {
        "id": "2hp5tATy10fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using min_length and max_length\n",
        "The pipeline() function, has two important parameters: min_length and max_length. These are useful for adjusting the length of the resulting summary text to be short, longer, or within a certain number of words."
      ],
      "metadata": {
        "id": "chagl2Ba2TcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a short summarizer\n",
        "short_summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\", min_length=1, max_length=10)\n",
        "# Summarize the input text\n",
        "short_summary_text = short_summarizer(text)\n",
        "# Print the short summary\n",
        "print(short_summary_text[0][\"summary_text\"])\n",
        "# Repeat for a long summarizer\n",
        "long_summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\", min_length=50, max_length=150)\n",
        "long_summary_text = long_summarizer(text)\n",
        "# Print the long summary\n",
        "print(long_summary_text[0][\"summary_text\"])"
      ],
      "metadata": {
        "id": "E0ZDxv5o2WL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing text with AutoTokenizer\n",
        "AutoTokenizers simplify text preparation by automatically handling cleaning, normalization, and tokenization. They ensure the text is processed just as the model expects."
      ],
      "metadata": {
        "id": "atKBqKqeSIxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary library for tokenization\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# Split input text into tokens\n",
        "tokens = tokenizer.tokenize(\"AI: Making robots smarter and humans lazier!\")\n",
        "\n",
        "# Display the tokenized output\n",
        "print(f\"Tokenized output: {tokens}\")"
      ],
      "metadata": {
        "id": "BOjfwCiNSKqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using AutoClasses\n",
        "In this code, we will combine AutoModels and AutoTokenizers with the pipeline() function. It's a nice balance of control and convenience.\n",
        "\n",
        "- Download the model and tokenizer and save as my_model and my_tokenizer, respectively\n",
        "\n",
        "- Create the pipeline and save as my_pipeline\n",
        "\n",
        "- Predict the output using my_pipeline and save as output"
      ],
      "metadata": {
        "id": "svoZewN7TwzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the model and tokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "my_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "my_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# Create the pipeline\n",
        "my_pipeline = pipeline(task=\"sentiment-analysis\", model=my_model, tokenizer=my_tokenizer)\n",
        "\n",
        "# Predict the sentiment\n",
        "output = my_pipeline(\"This course is pretty good, I guess.\")\n",
        "print(f\"Sentiment using AutoClasses: {output[0]['label']}\")"
      ],
      "metadata": {
        "id": "DMZpowCbTyiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting text with PyPDF\n",
        "PyPDF lets us extract text from PDFs, making it easy to work with multi-page documents like policy files.\n",
        "\n",
        "In this exercise, you’ll load the US_Employee_Policy.pdf, extract its content page by page, and combine it into a single string, preparing the text for a question-answering pipeline."
      ],
      "metadata": {
        "id": "_HGmClUZVf4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "from pypdf import PdfReader\n",
        "\n",
        "# Extract text from the PDF\n",
        "reader = PdfReader(\"AI Agents vs. Agentic AI.pdf\")\n",
        "\n",
        "# Extract text from all pages\n",
        "document_text = \"\"\n",
        "for page in reader.pages:\n",
        "    document_text += page.extract_text()\n",
        "\n",
        "print(document_text)"
      ],
      "metadata": {
        "id": "fH1i34AMVh1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Q&A pipeline\n",
        "\n",
        "You’ll build a question-answering pipeline using Hugging Face to retrieve specific answers from the document."
      ],
      "metadata": {
        "id": "2T_WI8_IXzWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "from pypdf import PdfReader\n",
        "\n",
        "# Extract text from the PDF\n",
        "reader = PdfReader(\"AI Agents vs. Agentic AI.pdf\")\n",
        "\n",
        "# Extract text from all pages\n",
        "document_text = \"\"\n",
        "for page in reader.pages:\n",
        "    document_text += page.extract_text()\n",
        "\n",
        "# Load the question-answering pipeline\n",
        "qa_pipeline = pipeline(task=\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "\n",
        "question = \"What is Negative Sample Integration?\"\n",
        "\n",
        "# Get the answer from the QA pipeline\n",
        "result = qa_pipeline(question=question, context=document_text)\n",
        "\n",
        "# Print the answer\n",
        "print(f\"Answer: {result['answer']}\")"
      ],
      "metadata": {
        "id": "UNvUXNnDX5FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4ZduoKOG2m1f"
      }
    }
  ]
}